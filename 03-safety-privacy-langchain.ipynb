{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety and Privacy with LangChain\n",
    "\n",
    "In this notebook we will use our privacy (PII entity recognizer and anonymizer) and toxic text classifier `tensor-trek/distilbert-toxicity-classifier` along with LangChain to implement checks of the text going into our LLM and text generated by the LLM. For this example we will use HuggingFace Hub LLM with LangChain and a custom `PrivacyAndSafetyChain` chain that implements the two checks.\n",
    "\n",
    "Note: You will need a HuggingFace Hub API Key for this notebook. Get your API Key from Huggingface hub - https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
    "\n",
    "Let's install some dependencies first\n",
    "- You will need `transformers`, PyTorch, `langchain`, `presidio-analyzer`, `presidio-anonymizer`, and `spacy` libraries\n",
    "- You will also need the spacy `en_core_web_lg` model. You can also work with `en_core_web_md` model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install torch torchvision\n",
    "!pip3 install langchain presidio-analyzer presidio-anonymizer spacy huggingface-hub\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the `PrivacyAndSafetyChain` custom chain\n",
    "\n",
    "The directory `PrivacyAndSafety` contains files that implements the custom Chain.\n",
    "- The `privacy_and_safety.py` file contains a Subclass of the base LangChain `Chain` class\n",
    "- The `check.py` file contains the actual toxic text classification and PII entity detection and anonymization\n",
    "\n",
    "Let's import and initialize `PrivacyAndSafetyChain` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrivacyAndSafety import PrivacyAndSafetyChain\n",
    "\n",
    "safety_privacy = PrivacyAndSafetyChain(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now initialized `PrivacyAndSafetyChain` with default options. We will test it out with a HuggingFace Hub hosted `google/flan-t5-xl` LLM with HuggingFace hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<YOUR HF TOKEN HERE>\"\n",
    "repo_id = \"google/flan-t5-xxl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PrivacyAndSafetyChain chain...\u001b[0m\n",
      "Running PrivacyAndSafetyChain...\n",
      "Toxic content found in text. Stopping...\n",
      "Toxic content found in text. Stopping...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 256}\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    prompt \n",
    "    | safety_privacy \n",
    "    | {\"input\": (lambda x: x['output'] ) | llm}\n",
    "    | safety_privacy \n",
    ")\n",
    "\n",
    "try:\n",
    "    response = chain.invoke({\"question\": \"What the hell is a machine learning algorithm?\"})\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "else:\n",
    "    print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Configuration for `PrivacyAndSafetyChain`\n",
    "\n",
    "We can customize the behavior of `PrivacyAndSafetyChain` via the following parameters\n",
    "\n",
    "- `pii_mask_character`, the character used to perform anonymization of PII entities. Default is `*`\n",
    "- `pii_labels` if you wish to specify a specific list of PII entity types, then a list of entity types. For a full list of PII entity labels refer [Presidio supported entities](https://microsoft.github.io/presidio/supported_entities/). Defaults to ALL entities.\n",
    "- `fail_on_pii` a boolean flag which will make the chain fail if PII is detected. Defaults to `False`.\n",
    "- `pii_threshold` the confidence score threshold for PII entity recognition. Defaults to 50%\n",
    "- `toxicity_threshold` the confidence score threshold for toxicity classification. Defaults to 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrivacyAndSafety import PrivacyAndSafetyChain\n",
    "\n",
    "safety_privacy = PrivacyAndSafetyChain(verbose=True, \n",
    "                                    pii_mask_character=\"#\",\n",
    "                                    pii_labels = [\"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"PERSON\", \"US_SSN\"],\n",
    "                                    fail_on_pii = True,\n",
    "                                    pii_threshold = 0.6,\n",
    "                                    toxicity_threshold = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anjanbiswas/.pyenv/versions/3.11.2/envs/langchain/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PrivacyAndSafetyChain chain...\u001b[0m\n",
      "Running PrivacyAndSafetyChain...\n",
      "Checking for Toxic content...\n",
      "Checking for PII...\n",
      "PII found and fail_on_pii is True. Stopping...\n",
      "PII found and fail_on_pii is True. Stopping...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 256}\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    prompt \n",
    "    | safety_privacy \n",
    "    | {\"input\": (lambda x: x['output'] ) | llm}\n",
    "    | safety_privacy \n",
    ")\n",
    "\n",
    "try:\n",
    "    response = chain.invoke({\"question\": \"\"\"What is John Doe's address, phone number and SSN from the following text?\n",
    "\n",
    "John Doe, a resident of 1234 Elm Street in Springfield, recently celebrated his birthday on January 1st. Turning 43 this year, John reflected on the years gone by. He often shares memories of his younger days with his close friends through calls on his phone, (555) 123-4567. Meanwhile, during a casual evening, he received an email at johndoe@example.com reminding him of an old acquaintance's reunion. As he navigated through some old documents, he stumbled upon a paper that listed his SSN as 338-12-6789, reminding him to store it in a safer place.\n",
    "\"\"\"})\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "else:\n",
    "    print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
